import os
import torch
import torch.nn as nn
import torch.optim as optim
from retrieval_net import RetrievalNet, CenterLoss
from hash_patch_dataset import get_dataloader
import config_hash as cfg

def hash_reg_loss(hash_codes):
    """
    Hashæ­£åˆ™é¡¹ï¼šé¼“åŠ±è¾“å‡ºé è¿‘Â±1ï¼Œä¾¿äºåç»­äºŒå€¼åŒ–
    """
    return ((hash_codes.abs() - 1) ** 2).mean()

def train():
    # è·å–è®¾å¤‡
    device = torch.device(cfg.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½è®­ç»ƒ/éªŒè¯æ•°æ®
    train_loader, classes = get_dataloader(cfg.train_data, cfg.batch_size, transform_type='train')
    val_loader, _ = get_dataloader(cfg.val_data, cfg.batch_size, transform_type='val')
    num_classes = len(classes)
    print(f"âœ… ç±»åˆ«æ•°: {num_classes}ï¼Œç±»å: {classes}")

    # åˆå§‹åŒ–æ¨¡å‹ä¸æŸå¤±å‡½æ•°
    model = RetrievalNet(hash_bits=cfg.hash_bits, num_classes=num_classes).to(device)
    ce_loss_fn = nn.CrossEntropyLoss()
    center_loss_fn = CenterLoss(num_classes, cfg.hash_bits).to(device)

    # è¶…å‚æ•°è®¾ç½®
    lambda_center = 0.01
    lambda_hash = 0.1

    # ä¼˜åŒ–å™¨ï¼ˆåŒæ—¶ä¼˜åŒ–æ¨¡å‹å’Œcenter lossçš„å‚æ•°ï¼‰
    optimizer = optim.Adam([
        {'params': model.parameters()},
        {'params': center_loss_fn.parameters(), 'lr': cfg.lr * 0.5}  # center lossç”¨è¾ƒå°å­¦ä¹ ç‡
    ], lr=cfg.lr, weight_decay=cfg.weight_decay)

    # è®­ç»ƒè¿‡ç¨‹
    best_acc = 0.0
    os.makedirs(cfg.checkpoint_dir, exist_ok=True)

    for epoch in range(cfg.epochs):
        model.train()
        total_loss = 0.0

        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            optimizer.zero_grad()

            hash_codes, logits = model(imgs)

            ce_loss = ce_loss_fn(logits, labels)
            reg_loss = hash_reg_loss(hash_codes)
            c_loss = center_loss_fn(hash_codes, labels)

            total_batch_loss = ce_loss + lambda_center * c_loss + lambda_hash * reg_loss
            total_batch_loss.backward()
            optimizer.step()

            total_loss += total_batch_loss.item()

        avg_train_loss = total_loss / len(train_loader)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device)
                _, logits = model(imgs)
                preds = torch.argmax(logits, dim=1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)
        val_acc = correct / total

        print(f"ğŸ“… Epoch [{epoch+1}/{cfg.epochs}] "
              f"- Loss: {avg_train_loss:.4f} "
              f"- Val Acc: {val_acc*100:.2f}%")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'model_state_dict': model.state_dict(),
                'center_state_dict': center_loss_fn.state_dict()
            }, os.path.join(cfg.checkpoint_dir, 'best_hash_model.pth'))
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ï¼ˆVal Acc: {best_acc*100:.2f}%ï¼‰")

    print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc*100:.2f}%")

if __name__ == '__main__':
    train()
