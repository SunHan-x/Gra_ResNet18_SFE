import os
import sys
# æ·»åŠ çˆ¶ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from datetime import datetime
from retrieval_net import SFDH_FGIR, CenterLoss
from hash_patch_dataset import get_dataloader
import config_hash as cfg

def quantization_loss(hash_codes):
    """
    é‡åŒ–æŸå¤±ï¼šé¼“åŠ±å“ˆå¸Œç æ¥è¿‘äºŒå€¼åŒ–å€¼
    """
    return torch.mean((hash_codes.abs() - 1) ** 2)

def train():
    # è·å–è®¾å¤‡
    device = torch.device(cfg.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºä¿å­˜è®­ç»ƒè®°å½•çš„ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join(cfg.checkpoint_dir, f'training_log_no_hash_{timestamp}')
    os.makedirs(save_dir, exist_ok=True)
    
    # ç”¨äºè®°å½•è®­ç»ƒæ•°æ®çš„DataFrame
    columns = ['epoch', 'total_loss', 'quant_loss', 'cls_loss', 'center_loss', 'val_acc']
    training_log = pd.DataFrame(columns=columns)

    # åŠ è½½è®­ç»ƒ/éªŒè¯æ•°æ®
    train_loader, classes = get_dataloader(cfg.train_data, cfg.batch_size, transform_type='train')
    val_loader, _ = get_dataloader(cfg.val_data, cfg.batch_size, transform_type='val')
    num_classes = len(classes)
    print(f"âœ… ç±»åˆ«æ•°: {num_classes}ï¼Œç±»å: {classes}")

    # åˆå§‹åŒ–æ¨¡å‹ä¸æŸå¤±å‡½æ•°
    model = SFDH_FGIR(
        hash_bits=cfg.hash_bits,
        num_classes=num_classes,
        loss_weight=[0, 1, 1]  # ä¸ä½¿ç”¨å“ˆå¸Œç›¸ä¼¼åº¦æŸå¤±ï¼Œä½¿ç”¨é‡åŒ–æŸå¤±å’Œåˆ†ç±»æŸå¤±
    ).to(device)
    
    ce_loss_fn = nn.CrossEntropyLoss()
    center_loss_fn = CenterLoss(num_classes, cfg.hash_bits).to(device)

    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam([
        {'params': model.parameters()},
        {'params': center_loss_fn.parameters(), 'lr': cfg.lr * 0.5}
    ], lr=cfg.lr, weight_decay=cfg.weight_decay)

    # ç”¨äºè®°å½•æŸå¤±å€¼çš„åˆ—è¡¨
    history = {
        'total_loss': [],
        'quant_loss': [],
        'cls_loss': [],
        'center_loss': [],
        'val_acc': []
    }

    # è®­ç»ƒè¿‡ç¨‹
    best_acc = 0.0
    os.makedirs(cfg.checkpoint_dir, exist_ok=True)

    for epoch in range(cfg.epochs):
        model.train()
        total_loss = 0.0
        total_quant_loss = 0.0
        total_cls_loss = 0.0
        total_center_loss = 0.0

        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            hash_codes, logits, _ = model(imgs)
            
            # è®¡ç®—å„ç±»æŸå¤±ï¼ˆé™¤äº†å“ˆå¸Œç›¸ä¼¼åº¦æŸå¤±ï¼‰
            quant_loss = quantization_loss(hash_codes)
            cls_loss = ce_loss_fn(logits, labels)
            center_loss = center_loss_fn(hash_codes, labels)

            # æ€»æŸå¤±
            loss = quant_loss + cls_loss + 0.01 * center_loss

            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡æŸå¤±
            total_loss += loss.item()
            total_quant_loss += quant_loss.item()
            total_cls_loss += cls_loss.item()
            total_center_loss += center_loss.item()

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / len(train_loader)
        avg_quant_loss = total_quant_loss / len(train_loader)
        avg_cls_loss = total_cls_loss / len(train_loader)
        avg_center_loss = total_center_loss / len(train_loader)

        # è®°å½•æŸå¤±å€¼
        history['total_loss'].append(avg_loss)
        history['quant_loss'].append(avg_quant_loss)
        history['cls_loss'].append(avg_cls_loss)
        history['center_loss'].append(avg_center_loss)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device)
                _, logits, _ = model(imgs)
                preds = torch.argmax(logits, dim=1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)
        val_acc = correct / total

        # è®°å½•å‡†ç¡®ç‡
        history['val_acc'].append(val_acc)

        # å°†å½“å‰epochçš„æ•°æ®æ·»åŠ åˆ°DataFrame
        epoch_data = pd.DataFrame([{
            'epoch': epoch + 1,
            'total_loss': avg_loss,
            'quant_loss': avg_quant_loss,
            'cls_loss': avg_cls_loss,
            'center_loss': avg_center_loss,
            'val_acc': val_acc
        }])
        training_log = pd.concat([training_log, epoch_data], ignore_index=True)

        # æ¯ä¸ªepochéƒ½ä¿å­˜ä¸€æ¬¡CSVæ–‡ä»¶
        training_log.to_csv(os.path.join(save_dir, 'training_log.csv'), index=False)

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        print(f"\nğŸ“… Epoch [{epoch+1}/{cfg.epochs}]")
        print(f"   Loss: {avg_loss:.4f} (Quant: {avg_quant_loss:.4f}, Cls: {avg_cls_loss:.4f}, Center: {avg_center_loss:.4f})")
        print(f"   Val Acc: {val_acc*100:.2f}%")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'center_state_dict': center_loss_fn.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
            }, os.path.join(cfg.checkpoint_dir, 'best_model_no_hash.pth'))
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼ï¼ˆVal Acc: {best_acc*100:.2f}%ï¼‰")

    print(f"\nğŸ¯ Training completed! Best validation accuracy: {best_acc*100:.2f}%")

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(15, 10))
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.subplot(2, 1, 1)
    plt.plot(history['total_loss'], label='Total Loss', color='red')
    plt.plot(history['quant_loss'], label='Quantization Loss', color='green')
    plt.plot(history['cls_loss'], label='Classification Loss', color='purple')
    plt.plot(history['center_loss'], label='Center Loss', color='blue')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Curves (No Hash Similarity Loss)')
    plt.legend()
    plt.grid(True)

    # ç»˜åˆ¶éªŒè¯å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(2, 1, 2)
    plt.plot(history['val_acc'], label='Validation Accuracy', color='orange')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Validation Accuracy Curve')
    plt.legend()
    plt.grid(True)

    # ä¿å­˜å›¾è¡¨
    plt.savefig(os.path.join(save_dir, 'training_curves.png'))
    plt.close()

if __name__ == '__main__':
    train() 